# Exno-2-Prompt-Engg

# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE: 23/04/2025                                                                            
### REGISTER NUMBER : 212222230112
 
###Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

### Algorithm:
Define the Use Case:
Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.
Create a Set of Prompts:
Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.
Run the Experiment on Each AI Platform:
Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.
Record response times, ease of interaction with the platform, and any technical issues encountered.
Evaluate Response Quality:
Assess each platformâ€™s responses using the following criteria: Accuracy,Clarity,Depth,Relevance 
Compare Performance:
Compare the collected data to identify differences in performance across platforms.
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.
Deliverables:
A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user 


### Algorithm: Design and Develop a Test scenario and execute the prompts under Diverse AI Platforms.

Sure! Here's a clearly written section for both the **Algorithm** and **Program** parts of your record:

---

### **Algorithm: Design and Develop a Test Scenario and Execute the Prompts under Diverse AI Platforms**

### Prompt:
"Explain how a Convolutional Neural Network (CNN) works and provide an example of its application in image classification."

### Expected Output from Different Platforms:
### Straightforward Prompts:
Description: Simple, direct requests that require a factual or clear response.

Example: "What is the capital of France?"

AI Performance:

ChatGPT: Quick, accurate responses, often with additional context or detail.

Claude: Provides clear, concise answers but sometimes adds nuance.

Bard: Responsive with direct answers but may include context that goes beyond the query.

Cohere Command: Effective for quick, straightforward answers with fewer embellishments.

Meta: Depends on the model used, but typically provides short and to-the-point responses.

### Tabular Format Prompting:
Description: Requests for data or information to be presented in a tabular format.

Example: "Create a table comparing the features of AI models."

AI Performance:

ChatGPT: Capable of structuring data in a readable table, offering detailed formatting.

Claude: Strong with structured data but may not always format neatly.

Bard: Can present structured data but may not always be in perfect tabular form.

Cohere Command: Effective in providing structured responses but may lack advanced formatting.

Meta: The output format can vary depending on the model, but typically straightforward.

### Missing Word Prompting:
Description: A prompt where a word or phrase is missing, and the AI needs to fill in the blank.

Example: "The capital of Italy is ___."

AI Performance:

ChatGPT: Can handle these efficiently and contextually.

Claude: Accurate, filling in the blank with appropriate words based on context.

Bard: Good at filling in the blanks with sensible choices.

Cohere Command: Performs well but might miss contextual subtleties.

Meta: Effectiveness depends on the exact question, but generally capable.

### Preceding Question Prompting:
Description: Prompts that refer to a previous question or statement.

Example: "Given that the capital of France is Paris, what is the capital of Italy?"

AI Performance:

ChatGPT: Strong memory handling, accurately processing the context of preceding queries.

Claude: Capable, but memory handling can sometimes be less consistent.

Bard: Good at making connections between questions but occasionally lacks continuity.

Cohere Command: Handles context moderately well, though it may miss nuanced connections.

Meta: Stronger when clear references are made, but may falter with long chains.

### Comparative Analysis Prompt:
Description: Requests that require comparing two or more entities, concepts, or ideas.

Example: "Compare the features of Python and Java."

AI Performance:

ChatGPT: Excellent at breaking down comparisons with detailed insights.

Claude: Provides detailed comparisons, emphasizing pros and cons.

Bard: Offers insightful, well-rounded comparisons.

Cohere Command: Efficient but may offer more concise comparisons without elaborating on each point.

Meta: Good for straightforward comparisons, but deeper analysis might be missed.

### Experiential Perspective Prompt:
Description: Prompts that ask for subjective or opinion-based responses, often based on personal experience.

Example: "What is the best way to improve focus while studying?"

AI Performance:

ChatGPT: Provides a balance of subjective and generalized advice.

Claude: Delivers nuanced responses, integrating empathy and reasoning.

Bard: Strong in providing practical and research-backed suggestions.

Cohere Command: Can offer solid advice but may lack a personal touch.

Meta: Provides responses based on general knowledge, lacking personal perspective.

### Everyday Functioning Prompts:
Description: Prompts that relate to common, everyday tasks or scenarios.

Example: "How do I prepare for a job interview?"

AI Performance:

ChatGPT: Provides detailed, step-by-step guides.

Claude: Offers practical advice, often more personalized.

Bard: Gives actionable, everyday advice, with examples.

Cohere Command: Functional and direct, but with less personalization.

Meta: Straightforward advice, focusing on general best practices.

### Universal Prompt Structures:
Description: Prompts with a flexible structure, designed to be broadly applicable.

Example: "Explain the process of machine learning in simple terms."

AI Performance:

ChatGPT: Excellent at tailoring explanations to different levels of complexity.

Claude: Very adaptable in offering explanations at multiple levels.

Bard: Good at offering universal explanations but may be verbose.

Cohere Command: Provides concise, clear explanations, especially for simple queries.

Meta: Generally strong in universal explanations, though may lack depth in complex topics.

### Summary of Responses:
| Prompt Type                     | **ChatGPT**                        | **Claude**                        | **Bard**                         | **Cohere Command**               | **Meta**                          |
|----------------------------------|-----------------------------------|----------------------------------|---------------------------------|----------------------------------|-----------------------------------|
| **Straightforward Prompts**      | Excellent, clear, and quick       | Clear and concise                | Responsive and detailed         | Efficient but less detailed      | Good, typically straightforward  |
| **Tabular Format Prompting**    | Excellent formatting capabilities | Moderate, occasionally messy     | Strong but occasionally incomplete | Less adept at formatting         | Varies, typically simple tables  |
| **Missing Word Prompting**      | Handles context well              | Accurate but less context-aware  | Good at filling blanks          | Solid, but misses nuances        | Effective for simple blanks      |
| **Preceding Question Prompting**| Strong memory for context         | Moderate memory handling         | Good for short context          | Works well with shorter context  | Can be inconsistent with longer chains |
| **Comparative Analysis Prompt** | Deep, insightful comparisons      | Good at breaking down pros/cons  | Balanced, insightful            | Concise but lacks depth          | Provides simple comparisons      |
| **Experiential Perspective Prompt** | Offers practical and empathetic responses | Nuanced and thoughtful          | Practical with real-world focus | Less personal, more factual      | General advice, lacks empathy    |
| **Everyday Functioning Prompts**| Detailed, step-by-step guides     | Practical, with personal touch   | Actionable, with examples       | Functional, straightforward      | Good general advice              |
| **Universal Prompt Structures** | Excellent at adapting to needs    | Versatile with multiple contexts | Strong explanations with examples| Concise, clear for simple topics | Clear, but lacks depth in complexity |
| **Prompt Refinements/Size Limitations** | Handles long prompts well      | Moderate handling of complex inputs | Works well with medium-length   | Handles long prompts, but less detail | Works well with medium-length    |

### Result:
Thus the Prompting tools are executed and analysed sucessfully .

